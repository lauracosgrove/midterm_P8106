---
title: "Predicting County-Level Heart Disease Mortality in the United States"
author: "Charlotte Abrams, Laura Cosgrove, Alyssa Vanderbeek"
date: "7 April 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(caret)
library(earth)
library(patchwork)
library(DALEX)
```

```{r, echo = FALSE}
heart <- read_rds("./data/full_processed_data.Rdata")

train <- read_rds("./data/train_imputed.Rdata") 
x <- model.matrix(heart_disease_mortality_per_100k ~ ., data = train)[,-1]
y <- train$heart_disease_mortality_per_100k

test = readr::read_rds("data/test_imputed.Rdata")
test_outcome = test$heart_disease_mortality_per_100k
test = model.matrix(heart_disease_mortality_per_100k ~ ., data = test)

# load the model fits from previous runs
lm_fit <- readRDS("lm_step_imputed.rds")
ridge_fit <- readRDS("ridge.rds")
mars_fit <- readRDS("mars2.rds")
gam_fit <- readRDS("gam_fit_imputed.rds")
lasso_fit <- readRDS("lasso_imputed.rds")
pcr_fit <- readRDS("pcr_imputed.rds")
```


## Introduction

Heart disease remains one of the leading causes of death in adults in the US. Understanding risk factors for diseases of this kind is an important task in working towards reducing the number of lives lost. There is obvious benefit in doing this at an individual level, examining personal lifestyle, genetic profile, and family history. But there may be important environmental components that have predictive value in assessing risk for heart disease mortality. Identifying county-level environmental and demographic predictor for heart disease mortality could aid in effective targeting of national campaigns to increase education and reduce risk. We examine economic, health, and demographic data for thousands of counties across the US. This data is synthesized from several sources, including the USDA Economic Research Service, Bureau of Labor Statistics, US Census, Behavioral Risk Factor Surveillance System, the CDC, and others. Our goal in this project is to most effectively predict the county-level heart disease mortality rate per 100,000 persons. We build 6 predictive models (stepwise linear regression, Lasso, Ridge, PCR, GAM, and MARS), and compare them on their predictive capacity quantified by the root mean squared error (RMSE).


## Methods

In our data pre-processing and exploratory analysis, we examined variable distributions (checking for skewness, missing data, etc). We eliminated two variables with over 90% missing data, and used K-nearest neighbors (KNN) to impute reminaing missing values. The dataset was split into training and testing subsets at a parsing 2:1 ratio. There was one categorical variable for which individual factor levels had near-zero variance, but because these levels were part of a larger parameter, we did not remove them. The final dataset includes the outcome (heart disease mortality rate per 100k) and 28 predictors relating to economic, demographic, and health-related county charateristics. 

We built predictive models on the training data using 6 methods: stepwise linear regression, Lasso, Ridge, PCR, GAM, and MARS. All model builds were performed using the `caret` package, and optimal model parameters were selected based on minimizing MSE (or, in the case of MARS, generalized cross-validation error) with 10-fold cross-validation, repeated five times. The optimal model was selected based on training RMSE, and models were again compared based on test RMSE. All R code used for model fitting is provided in the Appendix.

## Results

### Exploratory Data Analysis

The average county-level heart disease mortality per 100,000 residents is `r round(mean(heart$heart_disease_mortality_per_100k), 2)`, with standard deviation `r round(sd(heart$heart_disease_mortality_per_100k), 2)`. Though approximately normally distributed, the exact density of the outcome distribution differs slightly among levels of categorical variables [FIGURE].  

In the category of health, statistics related to lifestyle characteristics (obesity, smoking, diabetes, low birthweight babies, excessive drinking, and physical inactivity) are highly intercorrelated. Less so are more environmental - or "acts of God" - factors: air particulate matter, homocides, motor vehicle crashes, and rates of dentists and doctors (though the last two are highly correlated). For demography, we found categorical variables masquerading as separate predictors, leading to high intercorrelation within those categories: percentages of residents who are a given race accounts for 5 variables; age-related bins (less than 18, greater than 65) account for 2 variables; birth and death rate; and percentages of residents who complete a given level of education account for 4 variables. Given some of the subtleties in these distinctions, we condensed only race into "white" and "non-white". For economics, unsurprisingly, the percent of adults and the percent of children without health insurance  are highly correlated, as well as percent civilian labor and unemployment rate.

Between categories, education-related variables are highly correlated with % civilian labor, % uninsured adults, and % physical inactivity. Predictors strongly associated with heart disease mortality are % physical inactivity, % diabetes, % adult obesity, education-related variables, % low birthweight, and overall death rate per 1,000. Many of the variables most strongly-associated with the outcome are associated with one another.

### Predictive Models

```{r, echo=FALSE}
train <- read_rds("./data/train_imputed.Rdata") 
x <- model.matrix(heart_disease_mortality_per_100k ~ ., data = train)[,-1]
y <- train$heart_disease_mortality_per_100k

test_df = readr::read_rds("data/test_imputed.Rdata")
test_outcome = test_df$heart_disease_mortality_per_100k
test = model.matrix(heart_disease_mortality_per_100k ~ ., data = test_df)

# load the model fits from previous runs
lm_fit <- readRDS("lm_step_imputed.rds")
ridge_fit <- readRDS("ridge.rds")
mars_fit <- readRDS("mars2.rds")
gam_fit <- readRDS("gam_fit_imputed.rds")
lasso_fit <- readRDS("lasso_imputed.rds")
pcr_fit <- readRDS("pcr_imputed.rds")
```

_Final models and RMSE_ 

All models used the majority of all 28 variables (where categorical variables were split into dummy variables).

Table 1 presents the RMSE of the predicted heart disease mortality rate for all models on both the training and testing datasets. GAM outperforms all other models in both fields, though MARS has similar performce on predicting outcomes in the test data. All linear models have similar performance.

_Coefficient Shrinkage: Lasso and Ridge_

For better visualization of coefficient shrinkage among the possible , a `glmnet` model was fit and the lambda value selected by `caret` as minimizing RMSE through cross-validated iterations was plotted. 

_Investigation of MARS_

The minimum generalized cross-validation error was achieved for a total of 25 features, including a two-way interaction term. Variable importance is assessed by tracking GCV for each predictor and accumulating the reduction in GCV when each predictorâ€™s feature is added to the model -- the total reduction is used as the measure of variable importance. If a predictor was never used in any MARS basis function, it has an importance value of zero; 18 predictors were used in a MARS basis function.

MARS retained interactions between many sets of health-related variables, which one might expect given collinearity (and relatedness) between the within-category variable sets, as shown in [FIGURE]: Interaction terms were also present across categories.

_Investigation of GAM_

Compared to MARS and PCR, GAM gave more weight to the factor variable metropolitan status and metropolitan adjacency.

## Discussion

```{r include = FALSE}
set.seed(2)
# resample each model to compare 
res <- resamples(list(
  Stepwise = lm_fit,
  Ridge = ridge_fit,
  Lasso = lasso_fit,
  PCR = pcr_fit,
  GAM = gam_fit,
  MARS = mars_fit
  ))

cv_RMSE <- summary(res)$statistics$RMSE
gam_RMSE <- summary(res)$statistics$RMSE[5,]
```


GAM was our best-performing model. With GAM, we were able to achieve a cross-validated RMSE of median `r paste(round(gam_RMSE[3], 2))` and bootstrapped interquartile range of (`r paste(round(gam_RMSE[2], 2))`, `r paste(round(gam_RMSE[5], 2))`. GAM, and MARS, also performed well on the test data. 

All models had similar distributions of residuals, shown in [FIGURE].

This modeling problem was perhaps not best addressed by fitting lasso and ridge. Neither of the methods shrunk many coefficients in such a way that completely removed them from the model and improved the RMSE. The best visualization of the effect of shrinkage can be seen in the ridge coefficient plot [FIGURE], where it can be seen that the vast majority of coefficients shrunk equally. Lasso's more stringent shrinking power did not improve RMSE for this problem. Had these methods been more appropriate in this context, parameter selection might have been more obvious. As it is, as shown [IN FIGURE BLANK], the standard error of the mean cross-validated error is quite wide.

As described, GAM and MARS outperformed the linear models, and if the primary goal of the model is prediction, than we suggest using GAM. However, the linear models were close in performance to these more flexible models, and if simplicity and interpretability are major concerns, one of these may be preferred.

Variable importance was relatively consistent through each model, as measured by loss-perturbed RMSE in the `caret` cross-validation and shown in Appendix [FIGURE BLANK], generated by the package `DALEX`: for instance, overall death rate in a county consistently had a large, positive effect on the rate of heart disease mortality. However, compared to other models, GAM gave much more weight to the value of the geography-related factor variables metropolitan status (metro) and adjacency (metro_adjacency). Stepwise regression retained urban influence, and not other geography-related variables, and as we noted in exploratory data analysis, urban influence serves somewhate as a proxy for an interaction term between population and metropolitan status. Although the importance of particular predictors in a model with finite CV error-based parameter selection is subject to some randomness (or non-uniqueness), and therefore the meaning of importance should not be overinterpreted, it is possible that including several proxies for the same predictor type overcomplicated predictor selection, particularly in the lasso, ridge, and PCR cases. A different approach to try would be to begin with fewer factor predictors and note whether lasso, ridge, and PCR would have more effective shrinkage.

\pagebreak 

## Tables and Figures

```{r, echo = FALSE, fig.width = 10}
#Factor Variables
heart %>% 
  ggplot(aes(x = heart_disease_mortality_per_100k, y = metro, height = ..density..)) +
  ggjoy::geom_joy(scale = 0.85)

heart %>% 
  ggplot(aes(x = heart_disease_mortality_per_100k, y = metro_adjacency, fill = urban_influence, height = ..density..)) +
  ggjoy::geom_joy(scale = 0.85, alpha = 0.3) 

```

```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.width = 10}

##Functions
substring = function(x) {str_sub(x, start = -25)}

str_remove_hlth <- function(x) {
  str_remove(x, pattern = "health__")
}

str_remove_econ <- function(x) {
  str_remove(x, pattern = "econ__")
}

str_remove_demo <- function(x) {
  str_remove(x, pattern = "demo__")
}

str_remove_pct <- function(x) {
  str_remove(x, pattern = "pct_")
}

#Within category correlation
par(mfrow = c(1,3))

heart %>% 
  select_if(is.numeric) %>% 
  select(starts_with("econ")) %>% 
  rename_all(str_remove_econ) %>% 
  rename_all(str_remove_pct) %>% 
  rename_all(substring) %>% 
  drop_na() %>% 
  cor() %>% 
  corrplot::corrplot(method = "ellipse")

heart %>% 
  select_if(is.numeric) %>% 
  select(starts_with("demo")) %>% 
  rename_all(str_remove_demo) %>% 
  rename_all(str_remove_pct) %>% 
  rename_all(substring) %>% 
  drop_na() %>% 
  cor() %>% 
  corrplot::corrplot(method = "ellipse")

heart %>% 
  select_if(is.numeric) %>% 
  select(starts_with("health")) %>% 
  rename_all(str_remove_hlth) %>% 
  rename_all(str_remove_pct) %>% 
  rename_all(substring) %>% 
  drop_na() %>% 
  cor() %>% 
  corrplot::corrplot(method = "ellipse")

cor_matrix <- heart %>% 
  select_if(is.numeric) %>% 
  drop_na() %>% 
  cor()

cor_tibble = as_tibble(cor_matrix)

## Between category correlation
econ <- tibble(variable = names(cor_tibble),
                  econ__pct_civilian_labor = cor_tibble$econ__pct_civilian_labor,
       econ__pct_unemployment = cor_tibble$econ__pct_unemployment,
       econ__pct_uninsured_adults = cor_tibble$econ__pct_uninsured_adults,
       econ__pct_uninsured_children = cor_tibble$econ__pct_uninsured_children) %>% 
  filter(variable != "heart_disease_mortality_per_100k",
         !str_detect(variable, "econ")) %>% 
  arrange(variable) %>% 
  mutate(flag = case_when(abs(econ__pct_civilian_labor) > 0.6 ~ "flag",
                        abs(econ__pct_unemployment) > 0.6 ~ "flag",
                        abs(econ__pct_uninsured_adults) > 0.6 ~ "flag",
                        abs(econ__pct_uninsured_children) > 0.6 ~ "flag")) %>% 
  filter(flag == "flag") %>% 
  select(-flag)

health <- tibble(variable = names(cor_tibble),
                  health__air_pollution_particulate_matter = cor_tibble$health__air_pollution_particulate_matter,
       health__motor_vehicle_crash_deaths_per_100k = cor_tibble$health__motor_vehicle_crash_deaths_per_100k,
       health__pct_adult_obesity = cor_tibble$health__pct_adult_obesity,
       health__pct_adult_smoking = cor_tibble$health__pct_adult_smoking,
      health__pct_diabetes = cor_tibble$health__pct_diabetes,
       health__pct_low_birthweight = cor_tibble$health__pct_low_birthweight,
       health__pct_physical_inacticity = cor_tibble$health__pct_physical_inacticity,
       health__pop_per_dentist = cor_tibble$health__pop_per_dentist,
      health__pop_per_primary_care_physician = cor_tibble$health__pop_per_primary_care_physician) %>% 
  filter(variable != "heart_disease_mortality_per_100k",
         !str_detect(variable, "health")) %>% 
  arrange(variable) %>% 
  mutate(flag = case_when(abs(health__air_pollution_particulate_matter) > 0.6 ~ "flag",
                        abs(health__motor_vehicle_crash_deaths_per_100k) > 0.6 ~ "flag",
                        abs(health__pct_adult_obesity) > 0.6 ~ "flag",
                        abs(health__pct_adult_smoking) > 0.6 ~ "flag",
                      abs(health__pct_diabetes) > 0.6 ~ "flag",
                      abs(health__pct_low_birthweight) > 0.6 ~ "flag",
                      abs(health__pct_physical_inacticity) > 0.6 ~ "flag",
                      abs(health__pop_per_dentist) > 0.6 ~ "flag",
                      abs(health__pop_per_primary_care_physician) > 0.6 ~ "flag")) %>% 
  filter(flag == "flag")

econ <- econ %>% 
  gather(key = with, value = correlation, starts_with("econ")) %>% 
  filter(abs(correlation) > 0.6)

health <- health %>% 
  select(-flag) %>% 
  gather(key = with, value = correlation, starts_with("health")) %>% 
  group_by(variable) %>% 
  filter(abs(correlation) > 0.6)

multicollinearity <- full_join(econ, health)

multicollinearity %>% 
  knitr::kable(format = "latex", caption = "Highly Correlated Predictors Between Categories", digits = 2)

##Correlation with outcome
heart_attacks_cor = tibble(variable = names(cor_tibble),
                     correlation = cor_tibble$heart_disease_mortality_per_100k)

heart_attacks_cor %>% 
    arrange(desc(abs(correlation))) %>% 
    knitr::kable(format = "latex", caption = "Correlation with Outcome", digits = 2)

```


```{r, echo = FALSE}

# plot RMSE ad CI for each model to compare predictive ability
ggplot(res, metric = "RMSE") +
  theme_minimal() +
  labs(title = "Resampled Training RMSE")

pred_lm = predict(lm_fit, newdata = test)
pred_ridge = predict(ridge_fit, test)
pred_lasso = predict(lasso_fit, test)
pred_pcr = predict(pcr_fit, test)
pred_gam = predict(gam_fit, test)
pred_mars = predict(mars_fit, test)

data.frame(train_rmse = summary(res)$statistics$RMSE[,4],
           test_rmse = c(sqrt(mean((pred_lm - test_outcome)^2)),
                         sqrt(mean((pred_ridge - test_outcome)^2)),
                         sqrt(mean((pred_lasso - test_outcome)^2)),
                         sqrt(mean((pred_pcr - test_outcome)^2)),
                         sqrt(mean((pred_gam - test_outcome)^2)),
                         sqrt(mean((pred_mars - test_outcome)^2)))) %>%
  `colnames<-`(c("Train RMSE", "Test RMSE")) %>%
  knitr::kable(caption = "Predictive RMSE on training and test data")
```


```{r, echo = FALSE, warning = FALSE}

##Coefficient Shrinkage

### Ridge 
best_lambda_ridge = ridge_fit$bestTune$lambda 

ridge_cv_glmnet <- glmnet::cv.glmnet(x, y, 
                      alpha = 0, 
                      lambda = exp(seq(-2, 10, length = 200)), 
                      type.measure = "mse")

### Lasso 
best_lambda_lasso = lasso_fit$bestTune$lambda 

predict(lasso_fit$finalModel, s = best_lambda_lasso, type = "coefficients") %>% 
  dim() #no shrinkage

lasso_cv_glmnet <- glmnet::cv.glmnet(x, y, 
                      alpha = 1, 
                      lambda = exp(seq(-2, 4, length = 200)), 
                      type.measure = "mse")

plotmo::plot_glmnet(ridge_cv_glmnet$glmnet.fit, xvar = "lambda", label = 8) +
abline(v = log(best_lambda_ridge), col = "blue") + title(sub = "CV-selected lambda in blue", xlab = "Ridge")

plotmo::plot_glmnet(lasso_cv_glmnet$glmnet.fit, xvar = "lambda", label = 8) +
abline(v = log(best_lambda_lasso), col = "blue") + title(sub = "CV-selected lambda in blue", xlab = "Lasso")
#could suggest selection of a more parsimonious model: lambda = exp(0)

#### Some of the most "important" coefficients
predict(ridge_fit$finalModel, s = best_lambda_ridge, type = "coefficients") %>% 
  broom::tidy() %>% 
  arrange(desc(abs(value))) %>% 
  top_n(10) %>% 
  knitr::kable(format = "latex", caption = "Top Absolute-Values Coefficients, Ridge")

predict(lasso_fit$finalModel, s = best_lambda_lasso, type = "coefficients") %>% 
  broom::tidy() %>% 
  arrange(desc(abs(value))) %>% 
  top_n(10) %>% 
  knitr::kable(format = "latex", caption = "Top Absolute-Valued Coefficients, Lasso")
```

```{r, echo = FALSE}
## MSE 
### Ridge
ridge_cv_table <- tibble(lambda_values = ridge_cv_glmnet$lambda, mse = ridge_cv_glmnet$cvm, mse_upper = ridge_cv_glmnet$cvup, mse_lower = ridge_cv_glmnet$cvlo)

ridge_cv_table_best <- ridge_cv_table %>% 
  arrange(mse) %>% 
  slice(which.min(abs(lambda_values - best_lambda_ridge)))

ridge_mse_plot <- ridge_cv_table %>% 
  filter(lambda_values %in% exp(seq(-2, 5, length = 200))) %>% 
  ggplot(aes(x = log(lambda_values), y = mse)) +
  geom_point() +
  geom_point(aes(x = log(best_lambda_ridge), y = ridge_cv_table_best$mse), color = "red") +
  geom_ribbon(aes(ymin = mse_lower, ymax = mse_upper), alpha = 0.3) + 
  labs(title = "Ridge Parameter Selection") +
  annotate("text", x = 0.5 + log(best_lambda_ridge), y = 1.05*ridge_cv_table_best$mse, label = "Lambda chosen by caret", color = "red") + 
  theme_minimal()

### Lasso
lasso_cv_table <- tibble(lambda_values = lasso_cv_glmnet$lambda, mse = lasso_cv_glmnet$cvm, mse_upper = lasso_cv_glmnet$cvup, mse_lower = lasso_cv_glmnet$cvlo)

lasso_cv_table_best <- lasso_cv_table %>% 
  arrange(mse) %>% 
  slice(which.min(abs(lambda_values - best_lambda_lasso)))

lasso_mse_plot <- lasso_cv_table %>% 
  filter(lambda_values %in% exp(seq(-2, 0, length = 200))) %>% 
  ggplot(aes(x = log(lambda_values), y = mse)) +
  geom_point() +
  geom_point(aes(x = log(best_lambda_lasso), y = lasso_cv_table_best$mse), color = "red") +
  geom_ribbon(aes(ymin = mse_lower, ymax = mse_upper), alpha = 0.3) +
  labs(title = "Lasso Parameter Selection") + 
  theme_minimal()
library(patchwork)

ridge_mse_plot + lasso_mse_plot
```

```{r, echo = FALSE, fig.width = 10}
vip::vip(mars_fit, num_features = 18, bar = FALSE, value = "gcv") + ggtitle("GCV")

##Functions
str_remove_hlth <- function(x) {
  str_remove(x, pattern = "health__")
}

str_remove_econ <- function(x) {
  str_remove(x, pattern = "econ__")
}

str_remove_demo <- function(x) {
  str_remove(x, pattern = "demo__")
}

coef(mars_fit$finalModel) %>%
  broom::tidy() %>% 
  mutate(names = str_remove_hlth(names)) %>% 
  mutate(names = str_remove_hlth(names)) %>% 
  mutate(names = str_remove_econ(names)) %>% 
  mutate(names = str_remove_econ(names)) %>% 
  mutate(names = str_remove_demo(names)) %>% 
  mutate(names = str_remove_demo(names)) #%>% #repeated bc appears twice
 # knitr::kable(format = "latex")# %>% 
  #kableExtra::kable_styling(latex_options = "scale_down") 

p1 <- pdp::partial(mars_fit, pred.var = c("health__pct_physical_inacticity"), grid.resolution = 10) %>% autoplot()
p2 <- pdp::partial(mars_fit, pred.var = c("demo__pct_aged_65_years_and_older"), grid.resolution = 10) %>% autoplot()
p3 <- pdp::partial(mars_fit, pred.var = c("health__pct_diabetes"), grid.resolution = 10) %>% autoplot()
p4 <- pdp::partial(mars_fit, pred.var = c("health__pop_per_dentist"), grid.resolution = 10) %>% autoplot()

p5 <- pdp::partial(mars_fit, pred.var = c("health__pct_physical_inacticity", "demo__pct_aged_65_years_and_older"), grid.resolution = 10) %>%  pdp::plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, colorkey = TRUE, screen = list(z = -20, x = -60))
p6 <- pdp::partial(mars_fit, pred.var = c("health__pct_diabetes", "demo__pct_aged_65_years_and_older"), grid.resolution = 10) %>%  pdp::plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, colorkey = TRUE, screen = list(z = -20, x = -60))
p7 <- pdp::partial(mars_fit, pred.var = c("health__pct_diabetes", "health__pop_per_dentist"), grid.resolution = 10) %>%  pdp::plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, colorkey = TRUE, screen = list(z = -20, x = -60))


gridExtra::grid.arrange(p1, p2, p3, p5, p6, p7, ncol = 3)

```

```{r residuals, echo=FALSE, fig.cap = "Similar CDF of residuals by model"}
explainer_gam <- DALEX::explain(gam_fit, label = "gam", data = test, y = test_outcome)
explainer_mars <- DALEX::explain(mars_fit, label = "mars", data = test, y = test_outcome)
explainer_pcr <- DALEX::explain(pcr_fit, label = "pcr", data = test, y = test_outcome)
explainer_lasso <- DALEX::explain(lasso_fit, label = "lasso", data = test, y = test_outcome)
explainer_ridge <- DALEX::explain(lasso_fit, label = "ridge", data = test, y = test_outcome)
explainer_step <- DALEX::explain(lasso_fit, label = "step", data = test, y = test_outcome)

mp_gam <- model_performance(explainer_gam)
mp_mars <- model_performance(explainer_mars)
mp_pcr <- model_performance(explainer_pcr)
mp_lasso <- model_performance(explainer_lasso)
mp_ridge <- model_performance(explainer_ridge)
mp_step <- model_performance(explainer_step)

plot(mp_gam, mp_mars, mp_pcr, mp_lasso)
```

```{r variable importance, echo=FALSE, fig.cap = "GAM performance relies heavily on the weight of metropolitan status"}
explainer_gam <- DALEX::explain(gam_fit, label = "gam", data = test, y = test_outcome)
explainer_mars <- DALEX::explain(mars_fit, label = "mars", data = test, y = test_outcome)
explainer_pcr <- DALEX::explain(pcr_fit, label = "pcr", data = test, y = test_outcome)
explainer_lasso <- DALEX::explain(lasso_fit, label = "lasso", data = test, y = test_outcome)
explainer_ridge <- DALEX::explain(lasso_fit, label = "ridge", data = test, y = test_outcome)
explainer_step <- DALEX::explain(lasso_fit, label = "step", data = test, y = test_outcome)

vi_gam <- variable_importance(explainer_gam, loss_function = loss_root_mean_square)
vi_mars <- variable_importance(explainer_mars, loss_function = loss_root_mean_square)
vi_pcr <- variable_importance(explainer_pcr, loss_function = loss_root_mean_square)
vi_lasso <- variable_importance(explainer_lasso, loss_function = loss_root_mean_square)
vi_ridge <- variable_importance(explainer_ridge, loss_function = loss_root_mean_square)
vi_step <- variable_importance(explainer_step, loss_function = loss_root_mean_square)

plot(vi_gam, vi_mars, vi_pcr)
plot(vi_lasso, vi_ridge, vi_step)
```

```{r, echo = FALSE}
knitr::include_graphics("gam_s(18yrs_younger).png")
knitr::include_graphics("GAM_s(bachelors_or_higher).png")
```


\pagebreak

## Appendix

### Data prep

```{r, eval=F}
## Data import
predictors <- read_csv("./data/Training_values.csv") 
response <- read_csv("./data/Training_labels.csv") 


## Manipulation
data <- response %>% 
  full_join(predictors, by = "row_id") %>%
  separate(col = area__rucc, into = c('metro', 'population'), sep = ' - ') %>%
  rename(urban_influence = area__urban_influence,
         economic_typology = econ__economic_typology) %>%
  mutate(pure_population = fct_collapse(as.factor(population), 
                                        "more_than_1mil" = "Counties in metro areas of 1 million population or more",
                                        "250k_to_1mil" = "Counties in metro areas of 250,000 to 1 million population",
                                        "less_than_250k" = "Counties in metro areas of fewer than 250,000 population",
                                        "more_than_20k" = c("Urban population of 20,000 or more, adjacent to a metro area", 
                                                               "Urban population of 20,000 or more, not adjacent to a metro area"),
                                        "2500_to_20k" = c("Urban population of 2,500 to 19,999, adjacent to a metro area", 
                                                             "Urban population of 2,500 to 19,999, not adjacent to a metro area"),
                                        "less_than_2500" = c("Completely rural or less than 2,500 urban population, adjacent to a metro area", 
                                                        "Completely rural or less than 2,500 urban population, not adjacent to a metro area")),
         economic_typology = as.factor(recode(economic_typology,
                                              "Nonspecialized" = "Nonspecialized",
                                              "Manufacturing-dependent" = "Manufacturing",
                                              "Farm-dependent" = "Farming",
                                              "Federal/State government-dependent" = "Government",
                                              "Mining-dependent" = "Mining",
                                              "Recreation" = "Recreation")),
        metro = factor(metro, 
                       levels = c("Metro", "Nonmetro")),
        urban_influence = str_replace_all(urban_influence, " |/|-", "_"), # replace problematic characters
        urban_influence = str_replace_all(urban_influence, ",", ""), # replace problematic characters
        demo__pct_nonwhite = demo__pct_hispanic + demo__pct_asian + demo__pct_american_indian_or_alaskan_native + demo__pct_non_hispanic_african_american,
        urban_influence = fct_rev(urban_influence),
        metro_adjacency = fct_collapse(population, 
                                       metro = c("Counties in metro areas of 1 million population or more", 
                                                 "Counties in metro areas of 250,000 to 1 million population", 
                                                 "Counties in metro areas of fewer than 250,000 population"),
                                       adjacent = c("Urban population of 20,000 or more, adjacent to a metro area", 
                                                    "Urban population of 2,500 to 19,999, adjacent to a metro area", 
                                                    "Completely rural or less than 2,500 urban population, adjacent to a metro area"),
                                       nonadjacent = c("Urban population of 20,000 or more, not adjacent to a metro area", 
                                                       "Urban population of 2,500 to 19,999, not adjacent to a metro area", 
                                                       "Completely rural or less than 2,500 urban population, not adjacent to a metro area"))) %>% 
  dplyr::select(-demo__pct_hispanic, 
                -demo__pct_asian,
                -demo__pct_american_indian_or_alaskan_native, 
                -demo__pct_non_hispanic_african_american,
                -demo__pct_non_hispanic_white,
                -health__homicides_per_100k, # >90% missing
                -health__pct_excessive_drinking, # >90% missing
                -yr,
                -population,
                -row_id) 


```


Training and testing data split. Imputation on missing data.

```{r, eval=F}
## training/test data
set.seed(1)
train_ind <- sample(seq_len(nrow(data)), size = 2/3*nrow(data)) # select rows in 2:1 ratio 

train <- data[train_ind, ] # training dataset
test <- data[-train_ind, ] # testing dataset

# Imputation for missing values with caret, based on training data
training_preproc = caret::preProcess(train[,-1], 
                                     method = "knnImpute", # automatically centers and scales data
                                     pcaComp = 10,
                                     na.remove = TRUE,
                                     k = 5,
                                     knnSummary = mean,
                                     outcome = NULL,
                                     fudge = .2,
                                     numUnique = 3,
                                     verbose = TRUE)

# Impute training imputation on both training and testing datasets
train_imputed = predict(training_preproc, train)
test_imputed = predict(training_preproc, test)

#save files to Rdata: was not saving the factor structure in read from csv
saveRDS(train_imputed, file = './data/train_imputed.Rdata')
saveRDS(train_imputed, file = './data/test_imputed.Rdata')

```


*Linear Models*

Set up `caret` training control. We will use this for all models.

```{r}
set.seed(100)
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
```


Stepwise regression:
```{r, eval=F}
set.seed(2)
step.fit = caret::train(x, y, 
                        method = 'glmStepAIC',
                        metric = "RMSE",
                        trControl = ctrl)
saveRDS(step.fit, "lm_step_imputed.rds")
```


Lasso:
```{r, eval=F}
set.seed(2)
lasso_fit <- caret::train(x, y,
                          method = "glmnet",
                          metric = "RMSE",
                          tuneGrid = expand.grid(alpha = 1,
                                                lambda = exp(seq(-2, 4, length = 200))),
                          trControl = ctrl)
plot(lasso_fit, xTrans = function(x) log(x)) #in correct range

saveRDS(lasso_fit, "lasso_imputed.rds")
```


Ridge:
```{r, eval=F}
set.seed(100)

ridge_fit <- caret::train(x, y,
                     method = "glmnet",
                     tuneGrid = expand.grid(alpha = 0, 
                                            lambda = exp(seq(-2, 10, length = 200))),
                    trControl = ctrl1)


plot(ridge_fit, xTrans = function(x) log(x)) #in correct range

best_lambda_ridge = ridge_fit$bestTune$lambda 

saveRDS(ridge_fit, "ridge.rds")
```


PCR:
```{r, eval=F}
set.seed(2)
pcr_fit <- caret::train(x, y,
                        method = "pcr",
                        trControl = ctrl,
                        metric = "RMSE",
                        tuneLength = 200)
saveRDS(pcr_fit, "pcr_imputed.rds")
```



*Non-linear models*


GAM:
```{r, eval=F}
set.seed(2)
gam_fit <- caret::train(x, y, 
                        method = "gam",
                        metric = 'RMSE',
                        tuneGrid = data.frame(method = "GCV.Cp", select = c(TRUE, FALSE)),
                        trControl = ctrl)
summary(gam_fit)
saveRDS(gam_fit, "gam_fit_imputed.rds")

```


MARS:
```{r, eval=F}
mars_grid <- expand.grid(degree = 1:3, # degree: 1 vs 2 vs 3, no interaction vs. interaction;
                         nprune = 10:40) # nprune is number of coef

set.seed(2)

mars_fit <- caret::train(x, y,
                         method = "earth",
                         tuneGrid = mars_grid,
                         trControl = ctrl)

saveRDS(mars_fit, "mars.rds")

#based on initial results, choose parsimonious version
mars_grid_refined <- expand.grid(degree = 2, nprune = 25:40) 

mars_fit_refined <- caret::train(x, y,
                                 method = "earth",
                                 tuneGrid = mars_grid_refined,
                                 trControl = ctrl)
saveRDS(mars_fit_refined, "mars2.rds")

```

