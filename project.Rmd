---
title: "project"
author: "Laura Cosgrove"
date: "3/26/2019"
output: github_document
---
## Introduction
### Background (Alyssa)
### Exploratory Data Analysis (Laura)

## Linear Models

### Stepwise Selection (Alyssa)

### Lasso (Charlotte)

### Ridge (Laura)

### PCR (Charlotte)

## Nonlinear Models

### GAM (Alyssa)

### MARS (Laura)

## Classification Setting?

### KNN?

## Conclusions 

### Training RMSE

```{r}
set.seed(100)
library(tidyverse)
library(caret)

lm_fit <- readRDS("lm_step.rds")
ridge_fit <- readRDS("ridge.rds")
mars_fit <- readRDS("mars.rds")
gam_fit <- readRDS("gam_fit.rds")
lasso_fit <- readRDS("lasso.rds")
pcr_fit <- readRDS("pcr.rds")

res <- resamples(list(
  Stepwise = lm_fit,
  Ridge = ridge_fit,
  Lasso = lasso_fit,
  PCR = pcr_fit,
  GAM = gam_fit,
  MARS = mars_fit
  ))


ggplot(res, metric = "RMSE") +
  theme_minimal() +
  labs(title = "Resampled Training RMSE")
```

The more flexible model 

### Test RMSE (Alyssa)

**note: transform test data by centering and scaling with TRAINING means and standard deviations. Does predict do this automatically when using `preProcess` in caret?

can consider imputation: say 10%, one possible solution. use the training set to build. Look at the preProcess function, can use knn. 

Check/detect near-zero variance predictors: to decrease computational time and complexity.



### Interpretations (Laura)

_Coefficient Shrinkage: Lasso and Ridge_

_Investigation of MARS_

The minimum generalized cross-validation error was achieved

_Investigation of GAM_
